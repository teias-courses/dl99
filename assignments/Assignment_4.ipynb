{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wEtBmL1Hwk"
      },
      "source": [
        "# Assignment #04 - Sequential Models\n",
        "\n",
        "Deep Learning / Fall 1399, Khatam University\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_PRgRX1NDO"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "- **Assignment Due:** <b><font color='red'>1399.11.20</font></b> 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in <font color=\"purple\">**bold purple**</font> and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, **each student has to finish all the questions by him/herself**. If our matching system identifies any sort of copying, you'll be responsible for consequences.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Microsoft Teams channel.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- You can double click on collapsed code cells to expand them.\n",
        "- <b><font color='red'>When you are ready to submit, please follow the instructions at the end of this notebook.</font></b>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b201HiZiw5hB"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-szBDP6w7nC"
      },
      "source": [
        "In this assignment we are going to see some sequential models in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhGOa51Te7tp"
      },
      "source": [
        "#1.NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNhoCRD6xM5M"
      },
      "source": [
        "**Named Entity Recognition (NER)**, is the task of locating named entities mentioned in a given document. Therefore, this task is a simple sequence tagging task, in which the model must assign a class to every element of the sequence (in this case words, or to be more specific, tokens).\r\n",
        "\r\n",
        "In this assignment, we are going to implement and train a model for NER. We are going to get familiar with embedding layers, learn few practical tricks in implementing RNN models and address an issue regarding the imbalanced dataset.\r\n",
        "\r\n",
        "For this assignment, we are going to use the **CoNLL 2003** dataset. Let's first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqtfOW3wVc2d",
        "cellView": "form"
      },
      "source": [
        "#@title Donwload the dataset\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "!gdown https://drive.google.com/uc?id=1f4UfZdTnwQrgnJ_ppzk0vAEpQFEJDX9b\r\n",
        "!unzip NER_data.zip\r\n",
        "!rm NER_data.zip\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "print (\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4PoAVYqztSC"
      },
      "source": [
        "Let's see few lines of the training set file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjm1Bs1Hz0SN"
      },
      "source": [
        "!head -n 20 \"/content/NER_data/train.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egLwMxLhz6K_"
      },
      "source": [
        "As you can see, each file is a collection of documents, each document is a collection of sentences, and each sentence is split into several lines with each line representing a word and its corresponding info. Documents are separated with `-DOCSTART- -X- -X- O` lines and sentences are split with double newline characters. Each word comes with some additional information, but the parts we are interested in are the first part (which is the word itself) and the last part which is the word NER class. Let's parse and extract this information from the raw files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEFsucr2em_g",
        "cellView": "form"
      },
      "source": [
        "#@title Parse raw data files\r\n",
        "\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "def parse_input_file (file_path):\r\n",
        "  parsed_docs = []\r\n",
        "  with open (file_path, \"r\") as f:\r\n",
        "    docs = f.read().strip().split(\"\\n\\n\")\r\n",
        "    for doc in docs:\r\n",
        "      if \"-DOCSTART-\" in doc: continue\r\n",
        "      parsed_doc = []\r\n",
        "      words = doc.split(\"\\n\")\r\n",
        "      for word in words:\r\n",
        "        parts = word.split()\r\n",
        "        parsed_doc.append((parts[0], parts[-1]))\r\n",
        "      parsed_docs.append(parsed_doc)\r\n",
        "  return parsed_docs\r\n",
        "\r\n",
        "\r\n",
        "parsed_train= parse_input_file(\"/content/NER_data/train.txt\")\r\n",
        "parsed_test= parse_input_file(\"/content/NER_data/test.txt\")\r\n",
        "parsed_valid= parse_input_file(\"/content/NER_data/valid.txt\")\r\n",
        "  \r\n",
        "print (\"Done!\")\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZhhUHhC1Xlc"
      },
      "source": [
        "Now let's take a look at parsed datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-0Mekt1UMs"
      },
      "source": [
        "print (\"element 0 of parsed trainset:\\n\")\r\n",
        "parsed_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFFS7T212oI"
      },
      "source": [
        "As you can observe, each element in parsed datasets is a list of tuples, and each tuple represents a word and it's NER tag. Now, to feed the words and classes to a neural network, we need to map them to integer values. Let's create the mapping files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3AFmhtkhCMB",
        "cellView": "form"
      },
      "source": [
        "#@title Create mappings\r\n",
        "\r\n",
        "import string\r\n",
        "\r\n",
        "vocab = set()\r\n",
        "classes = set()\r\n",
        "\r\n",
        "\r\n",
        "for doc in parsed_train:\r\n",
        "  for word, label in doc:\r\n",
        "    vocab.add(word) #we could use python <set> datastructure\r\n",
        "    classes.add(label)\r\n",
        "\r\n",
        "word2id = {w:i for i, w in enumerate(vocab, 2)}\r\n",
        "word2id[\"<PAD>\"] = 0\r\n",
        "word2id[\"<UNK>\"] = 1\r\n",
        "\r\n",
        "tag2id = {c:i for i,c in enumerate(classes)}\r\n",
        "id2tag = {tag2id[c]:c for c in tag2id}\r\n",
        "\r\n",
        "english_chars = string.printable\r\n",
        "\r\n",
        "char2id = {c:i for i, c in enumerate(english_chars, 2)}\r\n",
        "char2id[\"<PAD>\"] = 0\r\n",
        "char2id[\"<UNK>\"] = 1\r\n",
        "\r\n",
        "print (\"Done!\")\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVNJFL4S2Tb1"
      },
      "source": [
        "Note that we added two special tokens `\"<PAD>\"` and `\"<UNK>\"` to our mapping files. `\"<UNK>\"` is useful when we are dealing with out of vocabulary words, and `\"<PAD>\"` is going to be used later to make all input sequences in a batch have the same length.\r\n",
        "\r\n",
        "Also, note that we have a mapping for characters as well as a mapping for words. This is because we are going to use a technique called **Character Embedding** in our implementation later.\r\n",
        "\r\n",
        "Now let's tokenize (map) our parsed datasets using these mappings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf-O45WLNDWk",
        "cellView": "form"
      },
      "source": [
        "#@title Tokenize\r\n",
        "\r\n",
        "def tokenize(parsed_data):\r\n",
        "\r\n",
        "  unk_tok = word2id[\"<UNK>\"]\r\n",
        "  unk_char = char2id[\"<UNK>\"]\r\n",
        "  word_tokenized_data = [ [word2id.get(w, unk_tok) for w,l in s] for s in parsed_data]\r\n",
        "  char_tokenized_data = [ [ [char2id.get(c, unk_char) for c in w] for w,l in s] for s in parsed_data]\r\n",
        "  tags = [ [tag2id[l] for w,l in s] for s in parsed_data]\r\n",
        "\r\n",
        "  return (word_tokenized_data,\r\n",
        "          char_tokenized_data,\r\n",
        "          tags)\r\n",
        "  \r\n",
        "tokenized_train = tokenize(parsed_train)\r\n",
        "tokenized_test = tokenize(parsed_test)\r\n",
        "tokenized_valid = tokenize(parsed_valid)\r\n",
        "\r\n",
        "print (\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u7hkyrp--jG"
      },
      "source": [
        "We created two different tokenized datasets: (1) a word-level tokenized dataset and (2) a char-level tokenized dataset. Let's take a look and see how they are constructed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGBwlMKAWNQ"
      },
      "source": [
        "word_tokenized_data, char_tokenized_data, tags = tokenized_train\r\n",
        "\r\n",
        "print(f\"Word level tokenized train dataset has {len(word_tokenized_data)} elements.\")\r\n",
        "print(f\"Each element represents a sentence and is stored as a {type(word_tokenized_data[0])}.\")\r\n",
        "print(f\"The first sentence contains {len(word_tokenized_data[0])} inner elemens, each representing a word (token).\")\r\n",
        "print(f\"Each of these tokens are a {type(word_tokenized_data[0][0])} representing a word id.\")\r\n",
        "\r\n",
        "print(\"\\n\"+55*\"-\"+\"\\n\")\r\n",
        "\r\n",
        "print(f\"Char level tokenized train dataset has {len(char_tokenized_data)} elements aswell.\")\r\n",
        "print(f\"Also, here each element represents a sentence and is stored as a {type(char_tokenized_data[0])}.\")\r\n",
        "print(f\"Also, here the first sentence contains {len(char_tokenized_data[0])} inner elemens, each representing a word (token).\")\r\n",
        "print(f\"However, here each word is represented by a {type(char_tokenized_data[0][0])} of character ids.\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU5TQxlPAGgp"
      },
      "source": [
        "The char-level tokenized dataset is going to be used later in our char-embedding representations.\r\n",
        "\r\n",
        "As we mentioned earlier, we have to make all sequences in a batch have the same length so we can feed them to a neural network. Therefore, we need to trim long sentences to `MAX_SENT_LEN`, and also pad the short ones to the same length. Likewise, for our char-level dataset, we also have to decide on a `MAX_WORD_LEN` to pad and trim the words. We also need to pad our labels. However, we did not have any specific class for padding, therefore we choose the \"O\" class for this purpose, and as we will see this is not important, we just need our labels to have a consistent length with our sentences.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A-tA0drkLBG",
        "cellView": "form"
      },
      "source": [
        "#@title pad and truncate\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "MAX_SENT_LEN = 48 #@param {type:\"integer\"}\n",
        "MAX_WORD_LEN = 16 #@param {type:\"integer\"}\n",
        "\n",
        "def pad_and_truncate(tokenized_data, sent_len=50, word_len=20):\n",
        "  word_tokenized, char_tokenized, tags = tokenized_data\n",
        "  pad_tok = word2id[\"<PAD>\"]\n",
        "  pad_char = char2id[\"<PAD>\"]\n",
        "  pad_cls = tag2id[\"O\"]\n",
        "  words_paded = pad_sequences(word_tokenized, \n",
        "                             maxlen=sent_len, \n",
        "                             padding='post', \n",
        "                             truncating='post',\n",
        "                             value=pad_tok)\n",
        "  \n",
        "  chars_paded = []\n",
        "\n",
        "  for doc in char_tokenized:\n",
        "    _temp = []\n",
        "    for word_index in range(sent_len):\n",
        "      if word_index < len(doc):\n",
        "        _temp.append(doc[word_index])\n",
        "      else:\n",
        "        _temp.append([])\n",
        "    doc_paded = pad_sequences(_temp, \n",
        "                              maxlen=word_len, \n",
        "                              padding='post', \n",
        "                              truncating='post',\n",
        "                              value=pad_char) \n",
        "    chars_paded.append(doc_paded)\n",
        "  chars_paded = np.array(chars_paded, \"int32\")\n",
        "\n",
        "      \n",
        "    \n",
        "  tags_paded = pad_sequences(tags, \n",
        "                              maxlen=sent_len, \n",
        "                              padding='post', \n",
        "                              truncating='post',\n",
        "                              value=pad_cls)\n",
        " \n",
        "\n",
        "  return words_paded, chars_paded, tags_paded\n",
        "\n",
        "paded_train = pad_and_truncate(tokenized_train, MAX_SENT_LEN, MAX_WORD_LEN)\n",
        "paded_test = pad_and_truncate(tokenized_test, MAX_SENT_LEN, MAX_WORD_LEN)\n",
        "paded_valid = pad_and_truncate(tokenized_valid, MAX_SENT_LEN, MAX_WORD_LEN)\n",
        "\n",
        "print (\"Done!\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI5OxdWQGQKs"
      },
      "source": [
        "Let's see the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVDxCKm9GPeD"
      },
      "source": [
        "word_paded_data, char_paded_data, tags_paded = paded_train\r\n",
        "\r\n",
        "print(f\"Paded word-level train dataset shape is: {word_paded_data.shape}\")\r\n",
        "print(f\"Paded char-level train dataset shape is: {char_paded_data.shape}\")\r\n",
        "print(f\"Paded train tags shape is: {tags_paded.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFZY5Yw7IVRs"
      },
      "source": [
        "Now let's make TensorFlow datasets! A regular TensorFlow dataset, returns a tuple of (inputs, label) per iteration, however, here we have a third element, which is `sample_weight`. The `sample_weight` indicates how much that sample contributes to our loss function. To obtain the `sample_weight` we pass a dictionary mapping each class to its corresponding weight to our dataset generation function. The goal is to prevent more frequent classes dominate and let less frequent classes be as impactful in the loss function. For now, we pass a default weight mapping and consider the weight of each class to be equal to one, but later on, you need to construct a proper weight mapping concerning the frequency of each class. Therefore notice how the default weigh dictionary is made here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QGxcui-T33i",
        "cellView": "form"
      },
      "source": [
        "#@title Tf datasets\r\n",
        "from tensorflow.data import Dataset\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "def make_tf_dataset (paded_data, class_weight):\r\n",
        "  words_paded, chars_paded, tags_paded = paded_data\r\n",
        "\r\n",
        "  words_dataset = Dataset.from_tensor_slices(words_paded)\r\n",
        "  chars_dataset = Dataset.from_tensor_slices(chars_paded)\r\n",
        "\r\n",
        "  X = Dataset.from_tensor_slices((words_paded, chars_paded))\r\n",
        "  tags_dataset = Dataset.from_tensor_slices(tf.one_hot(tags_paded, len(tag2id)))\r\n",
        "\r\n",
        "  sample_weights = []\r\n",
        "  for sentence in tags_paded:\r\n",
        "    sentence_weights = []\r\n",
        "    for tag in sentence:\r\n",
        "      sentence_weights.append(class_weight[tag])\r\n",
        "    sample_weights.append(sentence_weights)\r\n",
        "  sample_weights = Dataset.from_tensor_slices(sample_weights)\r\n",
        "\r\n",
        "  dataset = Dataset.zip ((X, tags_dataset, sample_weights)).batch(64).prefetch(tf.data.AUTOTUNE)\r\n",
        "\r\n",
        "  return dataset\r\n",
        "\r\n",
        "## Default weight mapping\r\n",
        "default_class_weights = {i:1 for i in id2tag}\r\n",
        "\r\n",
        "dataset_train = make_tf_dataset(paded_train, default_class_weights)\r\n",
        "dataset_test = make_tf_dataset(paded_test, default_class_weights)\r\n",
        "dataset_valid = make_tf_dataset(paded_valid, default_class_weights)\r\n",
        "\r\n",
        "print (\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk4TzUAYMpUi"
      },
      "source": [
        "Now it's time to make our model! As a baseline, we use a simple bi-LSTM neural network. Also, we just use the word level dataset (although the char level dataset is also passed to the model by the TensorFlow dataset we created, but we ignore it for this part). We follow these steps:\r\n",
        "\r\n",
        "\r\n",
        "1.   We take our input with shape `<batch, max_sent_length>` and feed it to an `Embedding` layer.\r\n",
        "2.   The `Embedding` layer maps each token id to a vector representation of that word. At first, these are random vectors, but as the training goes on, the model learns meaningful representations for each token. The output shape will be `<batch, max_sent_length, word_embedding_out>`. Note that we passed `True` to the `mask_zero` parameter of our `Embedding` layer. If you can recall, some layers take a mask tensor which indicates which inputs they should take into consideration when they do their computations. By activating `mask_zero`, our `Embedding` layer automatically generates a mask for tokens with id=0. This is the reason we considered the <PAD> token to be mapped to 0. The generated mask is then automatically passed to subsequent layers, and this is the reason why we arbitrarily padded our tags with class \"O\", since the mask is already generated and these padded values are masked regardless. \r\n",
        "\r\n",
        "3.   We feed the raw word representations to a `bi-LSTM` layer to create some context-aware representation for our words. The resulting shape will be <batch, max_sent_length, bi_LSTM_hidden_shape>\r\n",
        "\r\n",
        "4.   We finally pass these contextual representations to a `Dense` layer to tag our classes. The shape will be <batch, max_sent_length, class_num>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dUc3O7_xXDI",
        "cellView": "form"
      },
      "source": [
        "#@title base model architecture\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "class BaseNERModel(tf.keras.Model):\r\n",
        "    def __init__(self,\r\n",
        "                 max_sent_len,\r\n",
        "                 word_embed_input_dim, \r\n",
        "                 word_embed_output_dim,\r\n",
        "                 num_classes,\r\n",
        "                 hidden_size=75):\r\n",
        "      \r\n",
        "        super().__init__()\r\n",
        "        self.word_embedding = layers.Embedding(input_dim=word_embed_input_dim,\r\n",
        "                                               output_dim=word_embed_output_dim,\r\n",
        "                                               input_length=max_sent_len,\r\n",
        "                                               trainable=True,\r\n",
        "                                               mask_zero=True)\r\n",
        "\r\n",
        "\r\n",
        "        self.bilstm = layers.Bidirectional(layers.LSTM(hidden_size, return_sequences=True))\r\n",
        "        \r\n",
        "        self.dense = layers.Dense(num_classes, activation=\"softmax\")\r\n",
        "    \r\n",
        "    def call(self, inputs):\r\n",
        "        word_input, char_input = inputs\r\n",
        "        word_vectors = self.word_embedding(word_input)        \r\n",
        "  \r\n",
        "        lstm = self.bilstm(word_vectors) #batchsize, max_seq_len, hidden_dim_bilstm\r\n",
        "        logits = self.dense(lstm)\r\n",
        "\r\n",
        "        return logits\r\n",
        "\r\n",
        "base_model = BaseNERModel(max_sent_len=MAX_SENT_LEN,\r\n",
        "                          word_embed_input_dim=len(word2id), \r\n",
        "                          word_embed_output_dim=75,\r\n",
        "                          num_classes=len(tag2id))\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI0Lm2ircUSz"
      },
      "source": [
        "Let's train our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "S5y_H6PrSyg1"
      },
      "source": [
        "#@title train the base model\r\n",
        "\r\n",
        "base_model.compile(optimizer=\"adam\",\r\n",
        "              loss=\"categorical_crossentropy\",\r\n",
        "              metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "\r\n",
        "result = base_model.fit(dataset_train, epochs=8, validation_data=dataset_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DICtyoSscZMP"
      },
      "source": [
        "Now let's see how our baseline perfomed on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Qd4Ass_WS9yd"
      },
      "source": [
        "#@title predictions on test set\n",
        "sample_index = 200 #@param {type:\"integer\"}\n",
        "\n",
        "pred = base_model.predict(dataset_test)\n",
        "\n",
        "n = sample_index\n",
        "len_sent = len(parsed_test[n])\n",
        "print(\"{:15} | {:5} | {}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "print(32 * \"=\")\n",
        "for i in range (len_sent):\n",
        "  print(\"{:15} : {:5}   {}\".format(parsed_test[n][i][0], parsed_test[n][i][1], id2tag[np.argmax(pred[n][i], axis=-1)]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vcXAhVSdpwD"
      },
      "source": [
        "Evidently, **accuracy** is not a good performance metric in this case, since most of our instances are taged as \"O\" and the dataset is heavily unbalanced. Hence, we use the **F-score** to measure the performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "e9NazDcwS9ye"
      },
      "source": [
        "#@title f-score\r\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "## flaten the predictions and labels \r\n",
        "sent_lens = [min(len(s), MAX_SENT_LEN) for s in parsed_test] \r\n",
        "\r\n",
        "flat_test_preds = []\r\n",
        "for i, sent in enumerate(pred):\r\n",
        "  sent_preds = np.argmax(sent[:sent_lens[i]], axis=-1)\r\n",
        "  flat_test_preds += list(sent_preds)\r\n",
        "\r\n",
        "  \r\n",
        "flat_test_labels = []\r\n",
        "for i, sent in enumerate(parsed_test):\r\n",
        "  sent_labels = [tag2id[l] for w,l in sent]\r\n",
        "  flat_test_labels += sent_labels[:sent_lens[i]]\r\n",
        "\r\n",
        "## compute metric\r\n",
        "f1 = f1_score(flat_test_labels, flat_test_preds, average=None)\r\n",
        "\r\n",
        "print(\"{:12} |  {:5}\".format(\"Class\", \"F-score\"))\r\n",
        "print(25 * \"=\")\r\n",
        "for i in range (len(id2tag)):\r\n",
        "  print(\"{:12} : {:5.1f} \".format(id2tag[i], 100*f1[i]))\r\n",
        "print(25 * \"-\")\r\n",
        "print(\"{:12} : {:5.1f} \".format(\"AVG.\", 100*np.mean(f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLN_D9ZDekGT"
      },
      "source": [
        "<font color=\"purple\"><b>Now you must implement a model to use the char-level dataset as well. Follow these steps:</font></b>\r\n",
        "\r\n",
        "1.   <font color=\"purple\"><b>Generate raw word representations like before.</font></b>\r\n",
        "\r\n",
        "2.   <font color=\"purple\"><b>Use another `Embedding` layer which takes char-level dataset entries as input and generates raw character representations. Remember to set `mask_zero=True` to generate a mask for padded characters. The output shape will be `<batch, max_sent_length, max_word_length, char_embedding_out>`.</font></b>\r\n",
        "\r\n",
        "3.   <font color=\"purple\"><b>Use a 1D-convolutional layer on the char representations. This generates context-aware representations based on local features in raw subsequent character representations. The output shape will be `<batch, max_sent_length, num_conv_features, conv_feature_map>`.</font></b>\r\n",
        "\r\n",
        "4.   <font color=\"purple\"><b>We want to generate word representations using these context-aware character representations. Use a proper reduction method on the second axis (`num_conv_features`) to generate an output with shape `<batch, max_sent_length, conv_feature_map>`.</font></b>\r\n",
        "\r\n",
        "5. <font color=\"purple\"><b>Concatenate these newly generated word representations with the previous raw word representations you already had along the third axis. The output shape will be `<batch, max_sent_length, conv_feature_map + word_embeding_out>`.</font></b>\r\n",
        "\r\n",
        "6.  <font color=\"purple\"><b>Pass to `bi-LSTM` and `Dense` as before.</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TwAm6ayRt-H"
      },
      "source": [
        "#@title YOUR PART#1 - char level model architecture\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "class CharLevelNERModel(tf.keras.Model):\r\n",
        "  def __init__(self,\r\n",
        "                max_sent_len,\r\n",
        "                word_embed_input_dim, \r\n",
        "                word_embed_output_dim,\r\n",
        "                max_word_len,\r\n",
        "                char_embed_input_dim, \r\n",
        "                char_embed_output_dim,\r\n",
        "                num_classes,\r\n",
        "                conv_filters = 20,\r\n",
        "                hidden_size=75):\r\n",
        "    \r\n",
        "    ########################################\r\n",
        "    #     Put your implementation here     #\r\n",
        "    ########################################\r\n",
        " \r\n",
        "  def call(self, inputs):\r\n",
        "\r\n",
        "    ########################################\r\n",
        "    #     Put your implementation here     #\r\n",
        "    ########################################\r\n",
        "    \r\n",
        "char_level_model = CharLevelNERModel(max_sent_len=MAX_SENT_LEN,\r\n",
        "                          word_embed_input_dim=len(word2id), \r\n",
        "                          word_embed_output_dim=75,\r\n",
        "                          max_word_len=MAX_WORD_LEN,\r\n",
        "                          char_embed_input_dim=len(char2id), \r\n",
        "                          char_embed_output_dim=25,\r\n",
        "                          conv_filters = 20,\r\n",
        "                          num_classes=len(tag2id))\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMJx3xq6BjX3",
        "cellView": "form"
      },
      "source": [
        "#@title train the char level model\r\n",
        "\r\n",
        "char_level_model.compile(optimizer=\"adam\",\r\n",
        "              loss=\"categorical_crossentropy\",\r\n",
        "              metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "\r\n",
        "result = char_level_model.fit(dataset_train, epochs=8, validation_data=dataset_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frSy8TIGiQZn"
      },
      "source": [
        "Test the performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KbFHO7aCXsQJ"
      },
      "source": [
        "#@title f-score\r\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "\r\n",
        "pred = char_level_model.predict(dataset_test)\r\n",
        "\r\n",
        "## flaten the predictions and labels \r\n",
        "sent_lens = [min(len(s), MAX_SENT_LEN) for s in parsed_test] \r\n",
        "\r\n",
        "flat_test_preds = []\r\n",
        "for i, sent in enumerate(pred):\r\n",
        "  sent_preds = np.argmax(sent[:sent_lens[i]], axis=-1)\r\n",
        "  flat_test_preds += list(sent_preds)\r\n",
        "\r\n",
        "  \r\n",
        "flat_test_labels = []\r\n",
        "for i, sent in enumerate(parsed_test):\r\n",
        "  sent_labels = [tag2id[l] for w,l in sent]\r\n",
        "  flat_test_labels += sent_labels[:sent_lens[i]]\r\n",
        "\r\n",
        "## compute metric\r\n",
        "f1 = f1_score(flat_test_labels, flat_test_preds, average=None)\r\n",
        "\r\n",
        "print(\"{:12} |  {:5}\".format(\"Class\", \"F-score\"))\r\n",
        "print(25 * \"=\")\r\n",
        "for i in range (len(id2tag)):\r\n",
        "  print(\"{:12} : {:5.1f} \".format(id2tag[i], 100*f1[i]))\r\n",
        "print(25 * \"-\")\r\n",
        "print(\"{:12} : {:5.1f} \".format(\"AVG.\", 100*np.mean(f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to0lQd77iYNR"
      },
      "source": [
        "If you did everything right, the average f-score will be incresed by ~4 or 5  percents. \r\n",
        "\r\n",
        "<font color=\"purple\"><b>Explain why? What kind of information do you think is encoded in these newly word representaions generated using char representations?</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frdx64cujBlR"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER HERE! #####<b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IItrUMGfjFhw"
      },
      "source": [
        "As you can see our dataset is dominated by class \"O\". Here you must create a new class weight mapping with respect to each class frequency in our train set. The more frequent a class is, the less its weight should be.\r\n",
        "\r\n",
        "<font color=\"purple\"><b>Generate a proper class weighs mapping dictionary:<b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuX7BWzbJ9P3"
      },
      "source": [
        "#@title YOUR PART#2 - class weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_class_weight  = {} #?\r\n",
        "\r\n",
        "########################################\r\n",
        "#     Put your implementation here     #\r\n",
        "########################################\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOe0GpSmkmt8"
      },
      "source": [
        "Now let's initialize a new model and train again using weighted train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRx2eSXT6m9",
        "cellView": "form"
      },
      "source": [
        "#@title train the char level model with class weights\r\n",
        "\r\n",
        "dataset_train = make_tf_dataset(paded_train, train_class_weight)\r\n",
        "\r\n",
        "char_level_model = CharLevelNERModel(max_sent_len=MAX_SENT_LEN,\r\n",
        "                                      word_embed_input_dim=len(word2id), \r\n",
        "                                      word_embed_output_dim=75,\r\n",
        "                                      max_word_len=MAX_WORD_LEN,\r\n",
        "                                      char_embed_input_dim=len(char2id), \r\n",
        "                                      char_embed_output_dim=25,\r\n",
        "                                      num_classes=len(tag2id))\r\n",
        "\r\n",
        "char_level_model.compile(optimizer=\"adam\",\r\n",
        "              loss=\"categorical_crossentropy\",\r\n",
        "              metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "\r\n",
        "result = char_level_model.fit(dataset_train, epochs=8, validation_data=dataset_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jSCWtzaEUSik"
      },
      "source": [
        "#@title f-score\r\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "\r\n",
        "pred = char_level_model.predict(dataset_test)\r\n",
        "\r\n",
        "## flaten the predictions and labels \r\n",
        "sent_lens = [min(len(s), MAX_SENT_LEN) for s in parsed_test] \r\n",
        "\r\n",
        "flat_test_preds = []\r\n",
        "for i, sent in enumerate(pred):\r\n",
        "  sent_preds = np.argmax(sent[:sent_lens[i]], axis=-1)\r\n",
        "  flat_test_preds += list(sent_preds)\r\n",
        "\r\n",
        "  \r\n",
        "flat_test_labels = []\r\n",
        "for i, sent in enumerate(parsed_test):\r\n",
        "  sent_labels = [tag2id[l] for w,l in sent]\r\n",
        "  flat_test_labels += sent_labels[:sent_lens[i]]\r\n",
        "\r\n",
        "## compute metric\r\n",
        "f1 = f1_score(flat_test_labels, flat_test_preds, average=None)\r\n",
        "\r\n",
        "print(\"{:12} |  {:5}\".format(\"Class\", \"F-score\"))\r\n",
        "print(25 * \"=\")\r\n",
        "for i in range (len(id2tag)):\r\n",
        "  print(\"{:12} : {:5.1f} \".format(id2tag[i], 100*f1[i]))\r\n",
        "print(25 * \"-\")\r\n",
        "print(\"{:12} : {:5.1f} \".format(\"AVG.\", 100*np.mean(f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcNBN0i-kxfP"
      },
      "source": [
        "<font color=\"purple\"><b>Compare the results: which classes benefit and why?<b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Iejvn2lAAi"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER HERE! #####<b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0nMWRM2Bo3i"
      },
      "source": [
        "# 2. Sequence-to-Sequence Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ohFMPWBxM7"
      },
      "source": [
        "In the final part of the last assignment, we are going to try a different task type: sequence-to-sequence or shortly **seq2seq**. A seq2seq model simply gets a sequence of items as input and generates another sequence as its output. Its most common applications is machine translation, where seq2seq models were born. And you can think of other applications or search for them!\r\n",
        "\r\n",
        "Seq2seq tasks are different from sequence tagging because in seq2seq tasks, input and output items are not explicitly mapped to each other, and the length of input and output may not be equal at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_1R5vttJ1Pg"
      },
      "source": [
        "![](https://miro.medium.com/max/2400/1*1nERP8YPd-0DkpVC4Fi2pg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1cwAaVWGkvo"
      },
      "source": [
        "As you probably guessed, it is hard* to train a seq2seq model for a real-world problem like translation (why?). So we are going to apply a seq2seq model on a semi-toy problem: **Spelling Correction**. The input will be a text with spelling mistakes (sequence of characters) and the model should generate the corrected text (again as a sequence of chars). To prevent boredom and to exhilarate ourselves with literary delicacy and wisdom, the original texts are selected from Masnavi-e-Ma'navi. We will follow these steps to train our seq2seq model:\r\n",
        "\r\n",
        "1. Generate texts with controlled random spelling mistakes from original texts (mesra's of the poems)\r\n",
        "2. Prepare our data examples as pairs of input (noisy) and target (original) sequences\r\n",
        "3. Define our seq2seq model architecture\r\n",
        "4. Train the model on our generated data\r\n",
        "\r\n",
        "*requiring lots of computation resources, data and design effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiycxBodL76G"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBSfbtwzz99i"
      },
      "source": [
        "### Generate spelling mistakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT48dXF0sjsk"
      },
      "source": [
        "First question is \"how to make spelling mistakes as humans do?\" But we don't take this question too seriously! Probably we could use probabilistic models that imitate human mistakes, but we use a simplistic method to do that.\r\n",
        "\r\n",
        "In Persian, one main reason of spelling mistakes are **homophonic characters**, for example four different characters have same sound *z*! Other than that, typing errors are very frequent:\r\n",
        "* missing chars\r\n",
        "* inserting extra chars\r\n",
        "* hitting adjacent keys on keyboard\r\n",
        "* swaping order of adjacent chars\r\n",
        "\r\n",
        "Let's make it easier by taking homophonic characters and swaping characters from typos. We are going to define a function that makes random spelling mistakes of these two types..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozi-GcE0L6S"
      },
      "source": [
        "### Define Text Perturbation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6g76xJ8yF1z"
      },
      "source": [
        "\r\n",
        "Perturbation means making small changes to something, like adding noise to a signal. We are going to define perturbation functions that imitate spelling mistakes.\r\n",
        "\r\n",
        "It's not your turn yet :), so if not interested, you can skip reading and understanding the codes and just run the following cells and move forward..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R12bLo5IaXV0",
        "cellView": "form"
      },
      "source": [
        "#@title import libs\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "import re\r\n",
        "import os\r\n",
        "from random import random, choices, choice\r\n",
        "from collections import defaultdict, Counter\r\n",
        "import numpy as np\r\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STfSROK2WHz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e250d4aa-3ea9-4258-fee1-fd641196f966"
      },
      "source": [
        "# @title Word-level perterbations\r\n",
        "\r\n",
        "# persian keyboard layout, seems useless now!\r\n",
        "# but we could use it to model one of typing errors above...\r\n",
        "persian_kb = ['ضصثقفغعهخحجچپ',\r\n",
        "              'شسیبلاتنمکگ',\r\n",
        "              'ظطزرذدئو']\r\n",
        "\r\n",
        "persian_chars = ''.join(persian_kb) + 'ژآ'\r\n",
        "homophonic_groups = ['زذضظ', 'سصث', 'تط', 'غق', 'هح'] # + ['اآع']\r\n",
        "homophonic_chars = ''.join(homophonic_groups)\r\n",
        "\r\n",
        "def get_random_homophonic_char(ch):\r\n",
        "  for group in homophonic_groups:\r\n",
        "    if ch in group:\r\n",
        "      return choice(group.replace(ch, ''))\r\n",
        "  return ch\r\n",
        "\r\n",
        "def count_homophonic_chars(word):\r\n",
        "  homo_counts = Counter(homophonic_chars * 2)\r\n",
        "  char_counts = Counter(word)\r\n",
        "  return sum((homo_counts & char_counts).values())\r\n",
        "\r\n",
        "def homophonic_perturb(word):\r\n",
        "  is_homo = [1 if ch in homophonic_chars else 0 for ch in word]\r\n",
        "  new_word = list(word)\r\n",
        "  indexes = list(range(len(word)))\r\n",
        "  target_index = choices(indexes, weights=is_homo)[0]\r\n",
        "  new_char = get_random_homophonic_char(new_word[target_index])\r\n",
        "  new_word[target_index] = new_char\r\n",
        "  return ''.join(new_word)\r\n",
        "\r\n",
        "def swap_chars_perturb(word):\r\n",
        "  if len(word) < 2:\r\n",
        "    return word\r\n",
        "  new_word = list(word)\r\n",
        "  indexes = list(range(len(word)))\r\n",
        "  target_index = choice(indexes[:-1])\r\n",
        "  new_word[target_index], new_word[target_index+1] = new_word[target_index+1], new_word[target_index]\r\n",
        "  return ''.join(new_word)\r\n",
        "\r\n",
        "print('homophonic chars:', homophonic_perturb('صحیح'))\r\n",
        "print('swap adjacent chars:', swap_chars_perturb('صحیح'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "homophonic chars: ثحیح\n",
            "swap adjacent chars: حصیح\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "-evwjX7UfxYX",
        "cellView": "form",
        "outputId": "41acbdb1-45ec-4ff7-c83b-90a061287a84"
      },
      "source": [
        "# @title Text perturbation\r\n",
        "def perturb_words(words, weights, k, mode):\r\n",
        "  \"\"\"\r\n",
        "  perturbs words with given mode, k times.\r\n",
        "  \r\n",
        "  Arguments:\r\n",
        "  words: list of strings, the words to perturb\r\n",
        "  weights: list of weights for each word, used to selectd random word indexes\r\n",
        "  k: number of words to perturb\r\n",
        "  mode: perturbation mode, one of 'homophonic', 'swap'\r\n",
        "\r\n",
        "  Returns:\r\n",
        "  perturbed words as a list,\r\n",
        "  weight update which will be used to reduce probability of \r\n",
        "  perturbing a single word again in future pertarbations\r\n",
        "  \"\"\"\r\n",
        "  target_indexes = choices(range(len(words)), weights=weights, k=k)\r\n",
        "  weight_updates = np.ones(len(words))\r\n",
        "\r\n",
        "  for idx in target_indexes:\r\n",
        "    if mode == 'homophonic':\r\n",
        "      words[idx] = homophonic_perturb(words[idx])\r\n",
        "    elif mode == 'swap':\r\n",
        "      words[idx] = swap_chars_perturb(words[idx])\r\n",
        "    \r\n",
        "    weight_updates[idx] *= 0.1\r\n",
        "  return words, weight_updates\r\n",
        "\r\n",
        "def perturb_text(text,\r\n",
        "                 homophonic_prob=0.5, swap_prob=0.5):\r\n",
        "  \"\"\"\r\n",
        "  perturbs a given text, based on probabilities for each mode\r\n",
        "  \"\"\"\r\n",
        "  words = text.split(' ')\r\n",
        "\r\n",
        "  indexes = list(range(len(words)))\r\n",
        "  homophonic_weights = [count_homophonic_chars(word) for word in words]\r\n",
        "    \r\n",
        "  words, weights = perturb_words(words, homophonic_weights,\r\n",
        "                                 int(len(words)*homophonic_prob), mode='homophonic')\r\n",
        "  words, weights = perturb_words(words, weights,\r\n",
        "                                 int(len(words)*swap_prob), mode='swap')\r\n",
        "  return ' '.join(words)\r\n",
        "\r\n",
        "perturb_text('بیایید همیشه درست بنویسیم')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'بیایید حمیشه درست بنویثیم'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3TBimbH77Z"
      },
      "source": [
        "Now we have `perturb_text` function that applies spelling mistakes to the input text, controlled by the probability of each perturbation mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2acnFyA0HId"
      },
      "source": [
        "### Download, Read Texts\r\n",
        "\r\n",
        "Let's get our text data and check how our perturbation works on it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_T7oYyGA4UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "99b75814-e225-4007-b160-af1a3adcfd36"
      },
      "source": [
        "#@title Download texts\r\n",
        "!git clone https://github.com/ganjoor/ganjoor-tex\r\n",
        "# !git clone https://github.com/UniversalDependencies/UD_Persian-Seraji\r\n",
        "# !git clone https://github.com/tihu-nlp/normalized_bijankhan\r\n",
        "# !7z x /content/normalized_bijankhan/bijankhan.7z\r\n",
        "clear_output()\r\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFJVnHr12-b",
        "cellView": "form",
        "outputId": "edbdafb7-7966-46fd-b5da-c613d213c414"
      },
      "source": [
        "#@title Read texts\r\n",
        "\r\n",
        "import os, re\r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "  \"\"\"\r\n",
        "  cleans input text from non-Persian characters, ...\r\n",
        "  \"\"\"\r\n",
        "  oov_chars = re.compile(f'[^{persian_chars} ]+')\r\n",
        "  multispace = re.compile(f'[\\s]+')\r\n",
        "\r\n",
        "  text = text.replace('\\u200c', '')       # semi-space -> space\r\n",
        "  text = multispace.sub(' ', text)        # double-space -> space\r\n",
        "  text = oov_chars.sub('', text)          # remove out-of-vocab chars\r\n",
        "  return text.strip()\r\n",
        "\r\n",
        "def read_masnavi(data_dir):\r\n",
        "\r\n",
        "  for dname in os.listdir(data_dir):\r\n",
        "    directory = os.path.join(data_dir, dname)\r\n",
        "    if directory[-1] not in '12':\r\n",
        "      continue\r\n",
        "    for fname in os.listdir(directory):\r\n",
        "      with open(os.path.join(directory, fname)) as txt_file:\r\n",
        "        for i, text in enumerate(txt_file):\r\n",
        "          if i == 0:\r\n",
        "            continue\r\n",
        "          text = text.strip()\r\n",
        "          if len(text) > 10:\r\n",
        "            yield preprocess_text(text)\r\n",
        "\r\n",
        "def read_seraji(data_path):\r\n",
        "  with open(data_path) as txt_file:\r\n",
        "    for i, text in enumerate(txt_file):\r\n",
        "      if text.startswith('# text'):\r\n",
        "        text = text[8:]\r\n",
        "        if 10 < len(text) < 100:\r\n",
        "          yield preprocess_text(text)\r\n",
        "\r\n",
        "def read_bijankhan(data_path, max_char_len=64):\r\n",
        "  separators = '[#.؟؛)()]'\r\n",
        "  with open(data_path) as txt_file:\r\n",
        "    text = ''\r\n",
        "    for i, line in enumerate(txt_file):\r\n",
        "      if line.startswith('!'):\r\n",
        "        continue\r\n",
        "      elif line[0] in separators:\r\n",
        "        if len(preprocess_text(text)) > max_char_len / 2:\r\n",
        "          yield preprocess_text(text)\r\n",
        "        text = ''\r\n",
        "      else:\r\n",
        "        new_word = line.partition('\\t')[0] + ' '\r\n",
        "        if len(preprocess_text(text + new_word)) > max_char_len:\r\n",
        "          yield preprocess_text(text)\r\n",
        "          text = ''\r\n",
        "        text += new_word\r\n",
        "\r\n",
        "DATA_DIR = '/content/ganjoor-tex/txt/moulavi/masnavi' #@param {type: \"string\"}\r\n",
        "# DATA_DIR = '/content/bijankhan.txt'  #@param {type: \"string\"}\r\n",
        "MAX_LEN = 36  #@param {type: \"integer\"}\r\n",
        "# texts = list(read_bijankhan(DATA_DIR, MAX_LEN-2))\r\n",
        "texts = list(read_masnavi(DATA_DIR))\r\n",
        "\r\n",
        "counter = Counter()\r\n",
        "for text in texts:\r\n",
        "  counter.update(text.split())\r\n",
        "\r\n",
        "print(len(texts), 'texts')\r\n",
        "print(sum(map(lambda t: len(t.split()), texts)), 'word tokens')\r\n",
        "print(len(counter), 'unique words')\r\n",
        "print(sum(map(len, texts)), 'chars')\r\n",
        "print('\\nexample 0:\\n', texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15664 texts\n",
            "95897 word tokens\n",
            "13278 unique words\n",
            "397853 chars\n",
            "\n",
            "example 0:\n",
            " سوی مکه شیخ امت بایزید\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Reu_zP6TJZf2",
        "outputId": "a314bb42-bdaf-44da-c1db-8f597257016b"
      },
      "source": [
        "print(texts[1], '\\t',texts[0])\r\n",
        "print(perturb_text(texts[1]), '\\t', perturb_text(texts[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "از برای حج و عمره میدوید \t سوی مکه شیخ امت بایزید\n",
            "اض برای حج و عرمح مدیوید \t وسی مکح یشخ امط بایزید\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjKNmrok1kBY"
      },
      "source": [
        "### Make Tf data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KogsXR3lLPWo"
      },
      "source": [
        "Now we can make our training data as pair of perturbed (noisy) and original texts. But before proceeding, we need to define three special characters:\r\n",
        "* **start** of sequence, indicating the start of sequence!\r\n",
        "* **end** of sequence\r\n",
        "* **pad** character to extend our sequences to maximum length and ignore it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4rNpG6_8g6N"
      },
      "source": [
        "#@title Make char vocab, Define special chars\r\n",
        "pad_char = '_'\r\n",
        "start_char = '<'\r\n",
        "end_char = '>'\r\n",
        "all_chars = pad_char + start_char + end_char + ' ' + persian_chars\r\n",
        "\r\n",
        "inp_vocab_size = len(all_chars)\r\n",
        "trg_vocab_size = len(all_chars)\r\n",
        "\r\n",
        "id2ch = dict(enumerate(all_chars))\r\n",
        "ch2id = {ch:cid for cid, ch in id2ch.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOkuibskT9G8"
      },
      "source": [
        "Because our noisy input data is randomly generated, we want to generate new random inputs for each epoch. So we make our tf dataset using `tf.data.Dataset.from_generator`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_I_CAts3Zua",
        "cellView": "form"
      },
      "source": [
        "#@title Make tf data\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train_texts, val_texts = train_test_split(texts, test_size=0.1)\r\n",
        "\r\n",
        "BUFFER_SIZE = len(texts)\r\n",
        "MAX_LEN = 36  #@param {type: \"integer\"}\r\n",
        "BATCH_SIZE = 256 #@param {type: \"integer\"}\r\n",
        "\r\n",
        "steps_per_epoch = len(train_texts) // BATCH_SIZE\r\n",
        "\r\n",
        "def get_data_generator(texts):\r\n",
        "  def noisy_text_generator():\r\n",
        "    for text in texts:\r\n",
        "      noisy_text = perturb_text(text)\r\n",
        "\r\n",
        "      # add special characters and pad to max length\r\n",
        "      text = (start_char + text + end_char).ljust(MAX_LEN, pad_char)\r\n",
        "      noisy_text = (start_char + noisy_text + end_char).ljust(MAX_LEN, pad_char)\r\n",
        "\r\n",
        "      # char -> id\r\n",
        "      target_ids = [ch2id[ch] for ch in text]\r\n",
        "      noisy_ids = [ch2id[ch] for ch in noisy_text]\r\n",
        "      yield (tf.convert_to_tensor(noisy_ids, dtype=tf.int64),\r\n",
        "             tf.convert_to_tensor(target_ids, dtype=tf.int64))\r\n",
        "  return noisy_text_generator\r\n",
        "\r\n",
        "# training set\r\n",
        "dataset = tf.data.Dataset.from_generator(\r\n",
        "    get_data_generator(train_texts), \r\n",
        "    output_signature=(tf.TensorSpec((MAX_LEN,), dtype=tf.int64), tf.TensorSpec((MAX_LEN,), dtype=tf.int64)))\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "dataset = dataset.prefetch(BUFFER_SIZE)\r\n",
        "\r\n",
        "# validation set\r\n",
        "val_dataset = tf.data.Dataset.from_generator(\r\n",
        "    get_data_generator(val_texts), \r\n",
        "    output_signature=(tf.TensorSpec((MAX_LEN,), dtype=tf.int64), tf.TensorSpec((MAX_LEN,), dtype=tf.int64)))\r\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTnwZtFjJPtm",
        "outputId": "aee594b0-035c-43cc-8326-a69b6cca2301"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\r\n",
        "print('input batch shape (batch_size, input_seq_len):', example_input_batch.shape)\r\n",
        "print('target batch shape (batch_size, input_seq_len):', example_target_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input batch shape (batch_size, input_seq_len): (256, 36)\n",
            "target batch shape (batch_size, input_seq_len): (256, 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuFJ9ybaEAO3"
      },
      "source": [
        "## Define, Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1qOlpP7WNPb"
      },
      "source": [
        "We are going to implement a seq2seq model including:\r\n",
        "* Bidirectional GRU Encoder\r\n",
        "* Attention layer based on [Luong's](https://arxiv.org/pdf/1508.04025.pdf) Global attention\r\n",
        "* GRU Decoder \r\n",
        "\r\n",
        "The picture below shows the final model. We will implement these 3 part step by step and the attention layer is up to you!\r\n",
        "\r\n",
        "![](https://github.com/teias-courses/dl99/raw/gh-pages/assets/img/luong_att.png)\r\n",
        "\r\n",
        "Image is picked from [here](http://cnyah.com/2017/08/01/attention-variants/) and customized to fit our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfz_lf6VGmYQ",
        "cellView": "form"
      },
      "source": [
        "#@title model hyperparams (do not change!)\r\n",
        "\r\n",
        "EMBEDDING_DIM = 100 #@param {type: \"integer\"}\r\n",
        "UNITS = 300  #@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXX7JOkmkEQh"
      },
      "source": [
        "### Define Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXO3S1t8YF33"
      },
      "source": [
        "The encoder part has nothing new! A char embedding layer and a bidirectional GRU. We need encoder's final state (concatenation of forward and backward states) to later initialize the decoder with, and all output vectors $\\overline{h}_s$ as input to attention layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI9LodTTJiZF"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.units = units\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    gru_fw = tf.keras.layers.GRU(self.units//2,\r\n",
        "                                      return_sequences=True,\r\n",
        "                                      return_state=True)\r\n",
        "    gru_bw = tf.keras.layers.GRU(self.units//2,\r\n",
        "                                  return_sequences=True,\r\n",
        "                                  return_state=True,\r\n",
        "                                  go_backwards=True)\r\n",
        "    self.bi_gru = tf.keras.layers.Bidirectional(gru_fw, backward_layer=gru_bw)\r\n",
        "\r\n",
        "  def call(self, x, hidden):\r\n",
        "    x = self.embedding(x)\r\n",
        "\r\n",
        "    # split hidden state of forward and backward GRU\r\n",
        "    hidden = tf.split(hidden, num_or_size_splits=2, axis=-1)\r\n",
        "\r\n",
        "    output, state_fw, state_bw = self.bi_gru(x, initial_state=hidden)\r\n",
        "\r\n",
        "    # merge (concat) hidden state of forward and backward GRU\r\n",
        "    return output, tf.concat([state_fw, state_bw], axis=-1)\r\n",
        "\r\n",
        "  def initialize_hidden_state(self, batch_size):\r\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb-F_r34JlTK",
        "outputId": "7d9da8c6-185f-4195-b84d-60c3ec929799"
      },
      "source": [
        "encoder = Encoder(inp_vocab_size, EMBEDDING_DIM, UNITS)\r\n",
        "\r\n",
        "# sample input batch\r\n",
        "sample_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\r\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\r\n",
        "\r\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\r\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 36, 300)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG8ZgjyaYWv"
      },
      "source": [
        "Its time for you to take part and implement the attention layer. \r\n",
        "\r\n",
        "Attention mechanism generally has 3 main parts:\r\n",
        "* query vector\r\n",
        "* key vectors\r\n",
        "* value vectors\r\n",
        "\r\n",
        "The output will be a weighted average of **value vectors**, based on weights which we call them atterntion scores (or weights). The attention scores are calculated by a scoring function over **query** and **key vectors**.\r\n",
        "\r\n",
        "In our case the **query** is decoder state at current time step $h_t$ both **keys and values** are the same and equal to encoder outputs $\\overline{h}_s$.\r\n",
        "\r\n",
        "The score function proposed by Loung et al. is:\r\n",
        "\r\n",
        "$score(h_t, \\overline{h}_s)=h_t^\\top W_a\\overline{h}_s$\r\n",
        "\r\n",
        "where $W_a$ is a learnable parameter. Then the scores are sharpened using softmax:\r\n",
        "\r\n",
        "$a_t=sotfmax(score(h_t, \\overline{h}_s))$\r\n",
        "\r\n",
        "And the output of our attention layer (known as context vector) will be calculated as weighted average of $\\overline{h}_{s}$:\r\n",
        "\r\n",
        "${c}_t = \\sum\\limits_i a_{ti}\\overline{h}_{si}$\r\n",
        "\r\n",
        "<font color=\"purple\"><b>Now implement the `LuongAttention` layer which gets `query` and `values` (as a batch input)\r\n",
        "and returns both context vector $c_t$ and attention scores $a_t$ (for the input batch).</b></font>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umohpBN2OM94"
      },
      "source": [
        "#@title YOUR PART #3, Implement attention layer\n",
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # units == #units of encoder == #units of encoder (hidden size)\n",
        "    # and also size of the context vector we want to calculate\n",
        "    # define the learnable parameter here.\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query (decoder hidden state) shape: (batch size, hidden size)\n",
        "    # values shape: (batch size, sequence len, hidden size)\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    # context_vector shape: (batch size, hidden size)\n",
        "    # attention_scores shape: (batch size, sequence len)\n",
        "    return context_vector, attention_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k534zTHiDjQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fae4bd-991f-42ef-e02e-17e8b9303f8f"
      },
      "source": [
        "attention_layer = LuongAttention(UNITS)\n",
        "context_vector, attention_scores = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(context_vector.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length) {}\".format(attention_scores.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (256, 300)\n",
            "Attention weights shape: (batch_size, sequence_length) (256, 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAjC_VM5qwdv"
      },
      "source": [
        "The decoder is much more tricky! It includes:\r\n",
        "* a char embedding\r\n",
        "* a unidirectional GRU\r\n",
        "* the attention layer\r\n",
        "* a Dense layer \r\n",
        "\r\n",
        "On each `forward` call the decoder calculates one time step and returns next character as its main output, but there is much more in this single step to consider.\r\n",
        "\r\n",
        "The main part of decoder is a GRU. At each step, a GRU accepts an input vector, a hidden state (maybe its previous state) and returns an output vector (which equals its new hidden state). In the diagram below, you can see the flow of state vectors (horizontal) and input to output (vertical).\r\n",
        "\r\n",
        "![](https://github.com/teias-courses/dl99/raw/gh-pages/assets/img/luong_att.png)\r\n",
        "\r\n",
        "The decoder GRU recieves previous character from of decoder embedding, and also previous state of its own. It then makes an output vector, a new state.\r\n",
        "\r\n",
        "The state will be fed to GRU itself in future steps and also will be used to get context vector from attention layer $c_t$.\r\n",
        "\r\n",
        "The GRU output will be concatentated to context vector to make use of both attention context vector and decoder output $\\tilde{h}_t$. This vector is fed to dense layer to predict next character.\r\n",
        "\r\n",
        "And finally to make GRU aware of the decision made with help of the attention mechanism, $\\tilde{h}_t$ will be concatenated to input of GRU at next time step (dotted lines in the diagram).\r\n",
        "\r\n",
        "**NOTE: The flow of our decoder is mostly decided by the Luong's global attention mechansim. It may be different for other methods like the older [Bahdanau](https://arxiv.org/abs/1409.0473)'s attention.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.attention = LuongAttention(self.units)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "  def initialize_input_context(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units * 2))\n",
        "\n",
        "  def call(self, x, hidden, enc_output, previous_context):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding: (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation: (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(previous_context, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    gru_out, gru_state = self.gru(x, initial_state=hidden)\n",
        "    gru_out = gru_state   # when len(x) == 1 these have equal values, but output has time axis of size 1\n",
        "\n",
        "    context_vector, attention_weights = self.attention(gru_state, enc_output)\n",
        "\n",
        "    # output shape: (batch_size, hidden_size)\n",
        "    # gru_out = tf.reshape(gru_out, (-1, gru_out.shape[2]))\n",
        "    full_context = tf.concat([gru_out, context_vector], axis=1)\n",
        "\n",
        "    # output shape: (batch_size, vocab size + context dim)\n",
        "    y = self.fc(full_context)\n",
        "\n",
        "    return y, full_context, gru_state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e23883-fa71-45fe-852a-cb3e25a0da20"
      },
      "source": [
        "decoder = Decoder(trg_vocab_size, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "sample_decoder_output, _,  _, _ = decoder(\n",
        "    tf.zeros((BATCH_SIZE, 1)),\n",
        "    sample_hidden, sample_output,\n",
        "     decoder.initialize_input_context(BATCH_SIZE))\n",
        "\n",
        "print('Decoder output shape: (batch size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch size, vocab size) (256, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Define the optimizer, loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tXJ8df-Bxrn"
      },
      "source": [
        "To start training, first we need to define our loss function, optimizer. There is some trick for both of them:\r\n",
        "\r\n",
        "* The loss function is **sparse** cross entropy, because we do not convert targets to one-hot vectors.\r\n",
        "* There is no need to punish the model for pad target chars. So we define a custom loss function based on `SparseCategoricalCrossentropy` to mask out pad chars.\r\n",
        "* When training a model, it is often recommended to lower the learning rate as the training progresses. We do this using `ExponentialDecay` learning rate scheduler to start training fast and slowly converge in the end.\r\n",
        "* Do not try to change the learning rate value!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr",
        "cellView": "form"
      },
      "source": [
        "#@title Define optimizer, loss function (do not change LR!)\n",
        "LR = 0.002 #@param {type: \"number\"}\n",
        "\n",
        "sparse_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = sparse_cross_entropy(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# decays learning rate by 'decay_rate' after each 'decay_steps'\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    LR,\n",
        "    decay_steps=steps_per_epoch,\n",
        "    decay_rate=0.95)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training part has its own story! We train our model following these main steps:\n",
        "\n",
        "1. Pass the input through the encoder which return **encoder output** and the **encoder hidden state**.\n",
        "2. The **encoder output**, **encoder hidden state** and the **decoder input** (which is the `start_char`) is passed to the decoder.\n",
        "3. The decoder returns the **predictions**, **context vector** and the **decoder hidden state**.\n",
        "4. Predictions are used to calculate the loss and other decoder outputs are fed to itself to generate next character. For next time steps, the decoder input is edcided by **teacher forcing**.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LusV3OlTHu4t"
      },
      "source": [
        "<font color=\"purple\"><b>Read (and think) about teacher forcing in seq2seq models and explain why it is necessary to train seq2seq models? \r\n",
        "\r\n",
        "(It is associated with a single line in training step function below)</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPz2SuNnLUGF"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER HERE! #####<b></font>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn",
        "cellView": "form"
      },
      "source": [
        "#@title train step\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  \n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([ch2id[start_char]] * BATCH_SIZE, 1)\n",
        "    dec_input_context = decoder.initialize_input_context(BATCH_SIZE)\n",
        "    \n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_input_context, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, dec_input_context)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing: feeding the target as the next input\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLy-Z_GAtRyC",
        "cellView": "form"
      },
      "source": [
        "#@title validation step\r\n",
        "def val_step(inp, targ, enc_hidden):\r\n",
        "  \r\n",
        "  loss = 0\r\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n",
        "\r\n",
        "  dec_hidden = enc_hidden\r\n",
        "\r\n",
        "  dec_input = tf.expand_dims([ch2id[start_char]] * BATCH_SIZE, 1)\r\n",
        "  dec_input_context = decoder.initialize_input_context(BATCH_SIZE)\r\n",
        "\r\n",
        "  all_predictions = np.zeros((inp.shape))\r\n",
        "\r\n",
        "  for t in range(1, targ.shape[1]):\r\n",
        "    # passing enc_output to the decoder\r\n",
        "    predictions, dec_input_context, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, dec_input_context)\r\n",
        "\r\n",
        "    loss += loss_function(targ[:, t], predictions)\r\n",
        "    predicted_ids = tf.argmax(predictions, axis=-1)\r\n",
        "\r\n",
        "    # no teacher forcing: feeding its own preds to next step\r\n",
        "    dec_input = tf.expand_dims(predicted_ids, 1)\r\n",
        "\r\n",
        "    all_predictions[:, t] = predicted_ids.numpy()\r\n",
        "\r\n",
        "  batch_loss = (loss / int(targ.shape[1]))\r\n",
        "\r\n",
        "  return batch_loss, all_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "cellView": "form"
      },
      "source": [
        "#@title train loop\n",
        "EPOCHS = 20 #@param {type: \"integer\"}\n",
        "\n",
        "# metrics used for logging\n",
        "train_loss = tf.keras.metrics.Mean('train_loss')\n",
        "val_loss = tf.keras.metrics.Mean('val_loss')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  val_loss.reset_states()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
        "  \n",
        "  progress = tqdm(dataset, desc=f'Epoch {epoch+1}', total=steps_per_epoch)\n",
        "  for inp, targ in progress:\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss.update_state(batch_loss)\n",
        "\n",
        "    progress.set_postfix(loss=batch_loss.numpy())\n",
        "\n",
        "  for inp, targ in val_dataset:\n",
        "    batch_loss, _ = val_step(inp, targ, enc_hidden)\n",
        "    val_loss.update_state(batch_loss)\n",
        "\n",
        "  progress.set_postfix(\n",
        "      train_loss=train_loss.result().numpy(),\n",
        "      val_loss=val_loss.result().numpy(),\n",
        "      )\n",
        "  progress.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ZfkXcw0ZNx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ucm5428LjoP",
        "cellView": "form"
      },
      "source": [
        "#@title inference functions\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "def evaluate(text):\r\n",
        "  attention_map = np.zeros((MAX_LEN, MAX_LEN))\r\n",
        "\r\n",
        "  text = start_char + text + end_char\r\n",
        "\r\n",
        "  inputs = [ch2id[ch] for ch in text]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "      [inputs], maxlen=MAX_LEN, padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  hidden = tf.zeros((1, UNITS))\r\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\r\n",
        "\r\n",
        "  dec_hidden = enc_hidden\r\n",
        "  dec_input = tf.expand_dims([ch2id[start_char]], 0)\r\n",
        "  dec_input_context = decoder.initialize_input_context(1)\r\n",
        "\r\n",
        "  for t in range(MAX_LEN):\r\n",
        "    predictions, dec_input_context, dec_hidden, attention_weights = decoder(\r\n",
        "        dec_input,\r\n",
        "        dec_hidden,\r\n",
        "        enc_out,\r\n",
        "        dec_input_context)\r\n",
        "\r\n",
        "    # storing the attention weights to plot later on\r\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\r\n",
        "    attention_map[t] = attention_weights.numpy()\r\n",
        "\r\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\r\n",
        "\r\n",
        "    if id2ch[predicted_id] == end_char:\r\n",
        "      return result, text, attention_map\r\n",
        "\r\n",
        "    result += id2ch[predicted_id]\r\n",
        "\r\n",
        "    # the predicted ID is fed back into the model\r\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "  return result, text, attention_map\r\n",
        "\r\n",
        "def plot_attention(attentions, inputs, predicted):\r\n",
        "\r\n",
        "  fig = plt.figure(figsize=(10,10))\r\n",
        "  ax = fig.add_subplot(1, 1, 1)\r\n",
        "  attentions = attentions[:len(predicted),:len(inputs)]\r\n",
        "  ax.matshow(attentions, cmap='viridis')\r\n",
        "\r\n",
        "  fontdict = {'fontsize': 14}\r\n",
        "\r\n",
        "  ax.set_xticklabels(['_'] + list(inputs.ljust(MAX_LEN)), fontdict=fontdict)\r\n",
        "  ax.set_yticklabels(['_'] + list(predicted.ljust(MAX_LEN)), fontdict=fontdict)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdT3C7LMT3ND",
        "cellView": "form"
      },
      "source": [
        "#@title get output, plot attention\r\n",
        "text = '\\u0628\\u0634\\u0648\\u0646 \\u0627\\u0632 \\u0646\\u06CC \\u0686\\u0646\\u0648 \\u0647\\u06A9\\u0627\\u06CC\\u062A \\u0645\\u06CC\\u0646\\u06A9\\u062F'  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4JiyUZlVf_"
      },
      "source": [
        "<font color=\"purple\"><b>Now that you can feed the model with your arbitrary input, answer the following questions based on your observations of the outputs and attenion maps of the model. \r\n",
        "\r\n",
        "0. How can we make sure that the model does not memorize the training lyrics instead of correcting input spelling?\r\n",
        "1. When correcting homophonic perturbations, does it replace chars with the most frequent homophonic chars or considers the correct form of the words?\r\n",
        "2. Does the model fix spelling of a word based on its context? or it has leared something like a word dictionary?\r\n",
        "3. How the model handles swapped chars?\r\n",
        "4. Show some model failures (input, outputs) and explain why the model fails on that specific input?\r\n",
        "\r\n",
        "If the answer is based on the output you've got from the model, write that answer after a code cell showing your input to the model and the model output, attention map.<b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LwIkw91Z-c-h"
      },
      "source": [
        "text = ''  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd_mn4tv_MMM"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER TO Q0 HERE! #####<b></font>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ajjzrrXK-gr2"
      },
      "source": [
        "text = ''  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNG2zut_kfm"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER TO Q1 HERE! #####<b></font>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ap9nI2o-gzG"
      },
      "source": [
        "text = ''  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGjogAk_lT0"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER TO Q2 HERE! #####<b></font>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "trW-eoPO-g3-"
      },
      "source": [
        "text = ''  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knisT_GI_lr2"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER TO Q3 HERE! #####<b></font>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gbPZkPVE-g8e"
      },
      "source": [
        "text = ''  #@param {type:\"string\"}\r\n",
        "text = preprocess_text(text)\r\n",
        "\r\n",
        "result, text, attentions = evaluate(text)\r\n",
        "print(result)\r\n",
        "plot_attention(attentions, text, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJi37sg__mA0"
      },
      "source": [
        "<font color=\"purple\"><b>##### PUT YOUR ANSWER TO Q4 HERE! #####<b></font>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRra63i2oFS"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6iRsAn2rlE"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. **Fill your information** & run the cell bellow.\n",
        "4. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "5. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "6. Grab the downloaded file (`dl_asg04__xx__xx.zip`) and hand it over in microsoft teams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "## Fill your information (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5wTxj62CWc",
        "cellView": "form"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg04')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYJJJhh3kpj"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQc5tBQ2sFJ",
        "cellView": "form"
      },
      "source": [
        "#@title Make submission\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! pip install -U --quiet jdatetime > /dev/null\n",
        "\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import jdatetime\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'Assignment_4'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg04__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(jdatetime.date.today()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclPk2VM30Qa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "92c0935e-d2b6-4d14-a799-63d36986692a"
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3e723401-b910-43d1-a5d1-5152e9ad3c57\", \"dl_asg01__97722239__kiamehr_rezaee.zip\", 1780603)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}