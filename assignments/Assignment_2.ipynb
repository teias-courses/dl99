{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zDKgUvfw7gTO",
        "QilVSubqFXn4",
        "gtyiM-48N9Mm",
        "gncKjFdn8JRe",
        "hbGlplNwK1uF",
        "MqNDEGFeSGQ9",
        "SpULz3JZSQmB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wEtBmL1Hwk"
      },
      "source": [
        "# Assignment #02 - Regression with BackPropagation\n",
        "\n",
        "\n",
        "Deep Learning / Fall 1399, Khatam University\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_PRgRX1NDO"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "- **Assignment Due:** <b><font color='red'>1399/10/2</font></b> 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in <font color=\"purple\">**bold purple**</font> and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, **each student has to finish all the questions by him/herself**. If our matching system identifies any sort of copying, you'll be responsible for consequences.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Microsoft Teams channel.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- You can double click on collapsed code cells to expand them.\n",
        "- <b><font color='red'>When you are ready to submit, please follow the instructions at the end of this notebook.</font></b>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTZtiOeg1vHP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDKgUvfw7gTO"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrRyCdUFhlj"
      },
      "source": [
        "In this assignment, you will:\n",
        "- Get familiar with computational graphs and implement all parts required for creating one.\n",
        "- Use your implementations to solve a simple regression problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGBA6tmpkMS2"
      },
      "source": [
        "#@title Run This Cell\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QilVSubqFXn4"
      },
      "source": [
        "# 1 . Computational Graph Nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTSE8n3HHrc2"
      },
      "source": [
        "To make a computational graph, we need two main types of nodes: \n",
        "- **Value** nodes which are either our input values or trainable parameters,\n",
        "- and **Operation** nodes which are functions that take as input one or more value nodes or the output of other operations.\n",
        "\n",
        "So, let's implement these two nodes first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x57BadPMJnk"
      },
      "source": [
        "class Value():\n",
        "  def __init__(self, initial_value=None, trainable=True, graph=None):\n",
        "    self.value = initial_value\n",
        "    self.trainable = trainable\n",
        "    self.upstream_grad = np.array(0)\n",
        "    if graph:\n",
        "      graph.values.append(self)\n",
        "    else:\n",
        "      _default_graph.values.append(self)\n",
        "      \n",
        "  def set_value (self, new_value):\n",
        "    self.value = new_value\n",
        "\n",
        "  def get_value (self):\n",
        "    return self.value\n",
        "\n",
        "  def set_grad (self, grad):\n",
        "    self.upstream_grad = grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_K4bd8JQQ2"
      },
      "source": [
        "As you can see, each value node takes an initial_value as input. Moreover, it has a flag that determines whether this value is trainable (e.g. it is a model parameter) or non-trainable (e.g. it is a model input). We further pass a **graph** object to the value, which is the data structure in which we are going to add this value node. We implement the Graph class later, first let's implement a base class for **Operations** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGa8XxMyTB4Q"
      },
      "source": [
        "class Operation():\n",
        "\n",
        "  def __init__(self, inputs=None, graph=None):\n",
        "    self.inputs = inputs\n",
        "    self.output = None\n",
        "    self.upstream_grad = np.array(0)\n",
        "    if graph:\n",
        "      graph.operations.append(self)\n",
        "    else:\n",
        "      _default_graph.operations.append(self)\n",
        "  \n",
        "  def get_value (self):\n",
        "    return self.output\n",
        "  \n",
        "  def set_grad (self, grad):\n",
        "    self.upstream_grad = grad\n",
        "\n",
        "  def forward (self):\n",
        "    pass\n",
        "\n",
        "  def backward (self):\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdwblipaKubP"
      },
      "source": [
        "We can inherit from this base class and implement our desired operations. The `inputs` parameter is a list of inputs (each input can be either a Variable or another Operation) on which our operation is going to function. Implementation of `forward`  and `backward` methods is our main job when we are creating a custom operation. The `forward` method, is the logic of our operation and sets the value of the `output` attribute. The `backward` method is a function that is called during the back-propagation algorithm. To have a better sense, we first explain how the back-propagation algorithm works, and then we implement some custom operations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtyiM-48N9Mm"
      },
      "source": [
        "# 2 . Forward and Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU3BThSaNAXL"
      },
      "source": [
        "Let's say we have a simple linear regression problem. We have **(xi, yi)** pairs, and we want to find the optimal weight matrix **W** and the bias term **b** to minimize the **MSE loss** function:\n",
        "\n",
        "$$ MSE(X, Y) = \\frac{1}{N}\\sum_1^N {{(||WX + b - Y||}_2)^2}$$\n",
        "\n",
        " We can formulate the problem with the following graph:\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/uc?id=1WQvu1BwhrinTNBp_6I7aYQt6JEnX7xp6\" width=\"400\"/></p>\n",
        "\n",
        "To minimze the **loss** function, we can use gradient decent algorithm, but first we need to find the drivetive of **loss** with respect to **W** and **b** so we can change them in the direction opposite to the gradients. In order to do so, we can use the chain rule:\n",
        "\n",
        "- First we can calculate the gradient of **loss** function with respect to its direct inputs, e.g. **ADD** operation. We call this gradient $G_{ADD}(loss)$\n",
        "- Then we can calculate the gradient of **ADD** with respect to its direct input **b**, we call this gradient $G_{b}(ADD)$\n",
        "- Finally, we can calculate the gradient of **loss** with respect to **b** using the chain rule:\n",
        "<p align=\"center\"> $G_{b}(loss) = G_{ADD}(loss) . G_{b}(ADD)$\n",
        "</br>\n",
        "\n",
        "By expanding this idea, **back-propagation** calculates the gradient of the loss with respect to all parameters: \n",
        "\n",
        "- First, we need to calculate the loss and all its dependency nodes (these values might later going to be used in the calculation of some operation gradients). This step is called the **forward propagation** phase\n",
        "- Starting from the loss, we first calculate the derivative with respect to its direct inputs\n",
        "- We then pass these gradients down to these input nodes, and for each node, we calculate the derivative with respect to its direct inputs again, and we pas the upstream gradient multiplied by these new gradients down to the second level nodes\n",
        "- We continue this process until we reach leaf nodes.\n",
        "\n",
        "Check [this](http://neuralnetworksanddeeplearning.com/chap2.html) for a detailed explanation of the back-propagation algorithm.\n",
        "\n",
        "\n",
        "Now let's implement a simple binary-add operation to get a better sense:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxvEDqcgkFHI"
      },
      "source": [
        "class BinaryAdd(Operation):\n",
        "  \n",
        "  def __init__(self, inputs=None):\n",
        "    super().__init__(inputs)\n",
        "  \n",
        "  def forward(self):\n",
        "    a = self.inputs[0].get_value()\n",
        "    b = self.inputs[1].get_value()\n",
        "    self.output = a + b\n",
        "\n",
        "  def backward (self):\n",
        "    for inp in self.inputs:\n",
        "      grad_wrt_inp = self.upstream_grad\n",
        "      while np.ndim(grad_wrt_inp) > len(inp.get_value().shape):\n",
        "        grad_wrt_inp = np.sum(grad_wrt_inp, axis=0)\n",
        "      inp.set_grad(grad_wrt_inp)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RvYaMsl5PM"
      },
      "source": [
        "This operation gets a list with two inputs. As you can observe, the `forward` method simply adds the two inputs and sets the output value of the operation equal to the result, while the `backward` method computes the derivative of operation with respect to its inputs, multiplies these gradients with its received upstream gradient and sets the calculated multiplications as the upstream gradient of its respective input nodes. \n",
        "\n",
        "Let's implement another operation, a simple matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC009DuIpax7"
      },
      "source": [
        "class Matmul(Operation):\n",
        "  \n",
        "  def __init__(self, inputs=None):\n",
        "    super().__init__(inputs)\n",
        "  \n",
        "  def forward(self):\n",
        "    a = self.inputs[0].get_value()\n",
        "    b = self.inputs[1].get_value()\n",
        "    self.output = a.dot(b)\n",
        "\n",
        "  def backward (self):\n",
        "    a = self.inputs[0].get_value()\n",
        "    b = self.inputs[1].get_value()\n",
        "    self.inputs[0].set_grad(self.upstream_grad.dot(b.T))\n",
        "    self.inputs[1].set_grad(a.T.dot(self.upstream_grad))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx15cxZypd3Q"
      },
      "source": [
        "This operation takes as input a list of 2D-matrices (operations or values with output rank of 2) and calculates the matrix multiplication of them. The `forward` method is pretty straight-forward, but let's get into details of the `backward` method:\n",
        "\n",
        "- Let's say we feed a matrix **$X$** with shape **$m\\times{k}$** and a matrix **$W$** with shape **$k\\times{n}$** to the operation. The output **$O$** will be a matrix with the shape of **$m\\times{n}$**.\n",
        "\n",
        "- The upstream gradient **$G$** has the same shape as **$O$** (this holds for the upstream gradient of all the nodes since the upstream gradient is the gradient of the loss with respect to node outputs).\n",
        "\n",
        "- Let's say we want to calculate the gradient of loss with respect to  **$X$**: The upstream gradient with respect to the **$(i,j)$**-th element of **$X$** will be **$\\sum_{l=1}^n{G_{i,l}\\times{W_{j,l}}}$** (You can write this down to see it yourself). Therefore we can simply write the gradient with respect to **$X$** as **$G{W^{T}}$**. \n",
        "\n",
        "- Similarly, we can write the gradient with respect to **$W$** as **${X^{T}}G$**. \n",
        "\n",
        "<b><font color='purple'>Now it is your turn to implement the MSE operation</font></b>, which takse as input a list of 2D-matrices with the same shape, and calculates the following value as the output:\n",
        "\n",
        "</br>\n",
        "$$ MSE(X, Y) = \\frac{1}{N}\\sum_{all} {(X - Y)^2}$$\n",
        "\n",
        "In which **N** is the first dimension of **X** and **Y** (which is usually equal to batch size, if we consider our input shapes to be  $batch\\_size\\times{sample\\_size}$)\n",
        "\n",
        "- Note that if we want to use MSE as our loss, we can set the MSE upstream gradient to be equal to 1 since the gradient of the loss with respect to itself will be 1, but you should treat MSE as a regular operation in your implementation and take its upstream gradient into consideration regardless of whether we set it to 1 or not. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTbRXo84nWO"
      },
      "source": [
        "#@title Your Part #1\n",
        "class MSE(Operation):\n",
        "\n",
        "  def __init__(self, inputs=None):\n",
        "    super().__init__(inputs)\n",
        "  \n",
        "  def forward(self):\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "\n",
        "  def backward (self):\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gncKjFdn8JRe"
      },
      "source": [
        "#3 . Graph Iteration "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Pw2_4M8XRn"
      },
      "source": [
        "Now we implement the **Graph** class that we saw before. `Graph` is the data structure in which we append our operations and values. In order to add an operation or value into a `Graph` instance, we have to pass it when instantiating an `Operation` or a `Value` (refer to the implementation of them). If we call the `set_as_default` method of a `Graph` instance, nodes will be automatically added to that instance by default (refer to the implementation of `Operation` and `Value` again).\n",
        "\n",
        " `Graph` has two important methods: `forward_topology_ordering` and `backward_topology_ordering`. In order to evaluate the output of a node, we need to call its `forward` method, however the `forward` method of its inputs must have been called before this, and this goes recursively down until the inputs of a node are value nodes which their output is already prepared. Therefore, we need a list of nodes required to be called in order, starting from value nodes, before we can call the `forward` method of our target node. The `forward_topology_ordering` method finds this ordering for any given node. \n",
        " \n",
        "This procedure is reversed for the `backward` method: To call the `backward` method of a node, the `backward` method of its outputs must have been called, and so on. Hence `backward_topology_ordering`, finds the ordering of nodes in which we must call the `backward` methods, starting from a node back to the value nodes. Note that value nodes are not present in either ordering, since they have neither a `forward` method nor a `backward` method to be called. \n",
        "\n",
        "Here we implement `forward_topology_ordering` using a simple DFS algorithm. <b><font color='purple'> You must implement `backward_topology_ordering` using the BFS algorithm. </font></b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP_L0g8Vt53O"
      },
      "source": [
        "#@title Your Part # 2\n",
        "class Graph():\n",
        "  def __init__(self):\n",
        "    self.operations = []\n",
        "    self.values = []\n",
        "\n",
        "  def set_as_default(self):\n",
        "    global _default_graph\n",
        "    _default_graph = self\n",
        "\n",
        "  def forward_topology_ordering(self, operation):\n",
        "    ordering = []\n",
        "    visited_nodes = set()\n",
        "\n",
        "    def recursive_helper(node):\n",
        "      if isinstance(node, Operation):\n",
        "        for input_node in node.inputs:\n",
        "          if input_node not in visited_nodes:\n",
        "            recursive_helper(input_node)\n",
        "        ordering.append(node)\n",
        "      visited_nodes.add(node)\n",
        "\n",
        "    recursive_helper(operation)\n",
        "    return ordering\n",
        "\n",
        "  def forward(self, operation):\n",
        "    ordering = self.forward_topology_ordering(operation)\n",
        "    for node in ordering:\n",
        "      node.forward()\n",
        "    return operation.get_value()\n",
        "\n",
        "  def backward_topology_ordering(self, operation):\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "\n",
        "\n",
        "  def backward(self, loss_operation):\n",
        "\n",
        "    # The upstream gradient of loss function is always 1 since \n",
        "    # the derivative of loss with respect to its output is 1\n",
        "    loss_operation.set_grad (np.array(1))\n",
        "    ordering = self.backward_topology_ordering(loss_operation)\n",
        "    for node in ordering:\n",
        "      node.backward()\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbGlplNwK1uF"
      },
      "source": [
        "# 4 . Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMf2AuugK8ix"
      },
      "source": [
        "One last piece is remaining before we can complete our puzzle, which is the **Optimizer**. The `Optimizer` has the responsibility of changing the values of our model parameters with respect to their gradients. Each optimizer may have its own update rule, for example, a simple Gradient Decent optimizer simply changes the values in the opposite direction of the gradient with a step size which we call the learning rate. Here we implement a gradient descent optimizer, pay attention to the implementation details since you are going to implement another optimizer a bit later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78k9wybbK0Y2"
      },
      "source": [
        "class Optimizer():\n",
        "  def __init__ (self):\n",
        "    self.history = []\n",
        "\n",
        "  def reset_history(self):\n",
        "    self.history = []\n",
        "\n",
        "\n",
        "class GD(Optimizer):\n",
        "  def __init__ (self, learning_rate):\n",
        "    super().__init__()\n",
        "    self.learning_rate = learning_rate\n",
        "    \n",
        "  def optimize (self, graph, loss_operation):\n",
        "    ## Forward Propagation\n",
        "    l = graph.forward (loss_operation)\n",
        "    self.history.append(l)\n",
        "    ## Backward Propagation\n",
        "    graph.backward (loss_operation)\n",
        "    ## Variable Optimization\n",
        "    for v in graph.values:\n",
        "      if v.trainable:\n",
        "        v.set_value (v.get_value()-self.learning_rate*v.upstream_grad)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqNDEGFeSGQ9"
      },
      "source": [
        "# 5 . Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieEfFeeuOJE6"
      },
      "source": [
        "Nice, we now have all the pieces! Let's solve a simple linear regression problem to check if our implementations work.\n",
        "\n",
        "First, we create artificial data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRBixVQlOGc7"
      },
      "source": [
        "X = np.linspace(-10.0, 10.0, num=100)\n",
        "Y =  (2*(X-3)*(X+4)*(X) - X)+(np.random.rand((100))*500)-250\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(X, Y, \".\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4azHNhvSOyEW"
      },
      "source": [
        "Now let's find the best line we can draw to fit these data points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4VT1kszOxRM"
      },
      "source": [
        "G = Graph()\n",
        "G.set_as_default()\n",
        "\n",
        "initial_w = 40.0\n",
        "initial_b = -1500.0\n",
        "\n",
        "w = Value(initial_value=np.array([[initial_w]]))\n",
        "b = Value(initial_value=np.array([initial_b]))\n",
        "\n",
        "inp_x = Value(trainable=False)\n",
        "inp_y = Value(trainable=False)\n",
        "\n",
        "mul = Matmul([inp_x, w])\n",
        "add = BinaryAdd([mul, b])\n",
        "\n",
        "loss = MSE([add, inp_y])\n",
        "\n",
        "gd_opt = GD(0.0001)\n",
        "gd_opt.reset_history()\n",
        "\n",
        "for itter in tqdm(range (20000)):\n",
        "  inp_x.set_value (X.reshape((-1,1)))\n",
        "  inp_y.set_value (Y.reshape((-1,1)))\n",
        "\n",
        "  gd_opt.optimize (G, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_JsuOImPmuu"
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "X_0_0 , Y_0_0 = -10, -10*initial_w + initial_b\n",
        "X_100_0 , Y_100_0 = 10, 10*initial_w + initial_b\n",
        "ax[0].plot(X, Y, \".\")\n",
        "ax[0].plot([X_0_0, X_100_0], [Y_0_0, Y_100_0])\n",
        "ax[0].axis('off')\n",
        "ax[0].set_title(\"Before\")\n",
        "\n",
        "\n",
        "X_0_1 , Y_0_1 = -10, -10*w.value[0][0] + b.value[0]\n",
        "X_100_1 , Y_100_1 = 10, 10*w.value[0][0] + b.value[0]\n",
        "ax[1].plot(X, Y, \".\")\n",
        "ax[1].plot([X_0_1, X_100_1], [Y_0_1, Y_100_1])\n",
        "ax[1].axis('off')\n",
        "ax[1].set_title(\"After\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpULz3JZSQmB"
      },
      "source": [
        "# 6 . Gradient Descent with Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuIYijwmTUqI"
      },
      "source": [
        "There is an extension to the gradient descent called gradient descent with momentum. So instead of updating the parameters based only on current gradients, we also take into account the gradients from previous steps! \n",
        "\n",
        "$$ w \\leftarrow w - \\alpha (\\nabla_wLoss(i) + \\gamma  \\nabla_wLoss(i-1)) $$\n",
        "\n",
        "In which $\\alpha$ is our learning rate, $\\gamma$ is the momentum and $\\nabla_wLoss(i)$ denotes the gradient of $Loss$ with respect to $w$ at step $i$.\n",
        "\n",
        "<b><font color='purple'>Now it is your turn to implement gradient descent with momentum optimizer!</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsDvDW4FRJFJ"
      },
      "source": [
        "#@title Your Part #3\n",
        "class MomentumGD(Optimizer):\n",
        "  \n",
        "  def __init__ (self, learning_rate, momentum):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "\n",
        "  def optimize (self, graph, loss_operation):\n",
        "    ## Forward Propagation\n",
        "    l = graph.forward (loss_operation)\n",
        "    self.history.append(l)\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5_31y6RWy5Z"
      },
      "source": [
        "Now let's solve our regression problem with this new optimizer once again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izAPxk_WRRwf"
      },
      "source": [
        "G = Graph()\n",
        "G.set_as_default()\n",
        "\n",
        "initial_w = 40.0\n",
        "initial_b = -1500.0\n",
        "\n",
        "w = Value(initial_value=np.array([[initial_w]]))\n",
        "b = Value(initial_value=np.array([initial_b]))\n",
        "\n",
        "inp_x = Value(trainable=False)\n",
        "inp_y = Value(trainable=False)\n",
        "\n",
        "mul = Matmul([inp_x, w])\n",
        "add = BinaryAdd([mul, b])\n",
        "\n",
        "loss = MSE([add, inp_y])\n",
        "\n",
        "mgd_opt = MomentumGD(0.0001, 0.7)\n",
        "mgd_opt.reset_history()\n",
        "\n",
        "for itter in tqdm(range (20000)):\n",
        "  inp_x.set_value (X.reshape((-1,1)))\n",
        "  inp_y.set_value (Y.reshape((-1,1)))\n",
        "\n",
        "  mgd_opt.optimize (G, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smLEd8JDW7LX"
      },
      "source": [
        "Let's check how these optimizers perform compared to each other, by visualizing the loss history:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmpX4FXcRgti"
      },
      "source": [
        "plt.plot(range(20000), gd_opt.history[:], label=\"Regular\")\n",
        "plt.plot(range(20000), mgd_opt.history[:], label=\"With Momentum\")\n",
        "\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.xlabel('Itteration')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erc4VBjXXIBq"
      },
      "source": [
        "As you can observe, momentum gradient descent converges much faster.\n",
        "\n",
        "\n",
        "<b><font color='purple'>What is the reason?</font></b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R34SB-reXdDA"
      },
      "source": [
        "<b><font color='purple'> -- PUT YOUR ANSWER HERE! -- </font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRra63i2oFS"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Do2a0wXcE6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6iRsAn2rlE"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. **Fill your information** & run the cell bellow.\n",
        "4. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "5. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "6. Grab the downloaded file (`dl_asg02__xx__xx.zip`) and hand it over in microsoft teams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "## Fill your information (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5wTxj62CWc",
        "cellView": "form"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg02')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYJJJhh3kpj"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQc5tBQ2sFJ",
        "cellView": "form"
      },
      "source": [
        "#@title Make submission\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! pip install -U --quiet jdatetime > /dev/null\n",
        "\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import jdatetime\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'Assignment_2'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg02__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(jdatetime.date.today()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclPk2VM30Qa"
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}